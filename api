Project B: Remote SQL Explainer using llm crate v1.3.2 + OpenAI
Ideal for cloud-based inference via OpenAI API.

Cargo.toml

toml
Copy
Edit
[dependencies]
llm = "1.3.2"
clap = { version = "4.5", features = ["derive"] }
anyhow = "1.0"
tokio = { version = "1", features = ["full"] }
dotenv = "0.15"
src/main.rs

rust
Copy
Edit
mod llm;
use clap::{Parser, Subcommand};
use std::fs;

#[derive(Parser)]
struct Cli { #[command(subcommand)] cmd: Cmd }
#[derive(Subcommand)]
enum Cmd {
    Explain { #[arg(short, long)] t: Option<String>, #[arg(short, long)] f: Option<String> }
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    dotenv::dotenv().ok();
    let cli = Cli::parse();
    let sql = cli.cmd;
    let input = match sql { Cmd::Explain { t: Some(t), .. } => t,
                            Cmd::Explain { f: Some(f), .. } => fs::read_to_string(f)?,
                            _ => { eprintln!("Use -t or -f"); return Ok(()) }};
    let prompt = format!("Explain SQL:\n\n{}", input);
    let resp = llm::query_remote(&prompt).await?;
    println!("\nüåê Explanation:\n{}", resp);
    Ok(())
}
src/llm.rs

rust
Copy
Edit
use llm::backends::openai::OpenAIProvider;
use llm::builder::Builder;
use llm::completion::CompletionModel;
use anyhow::Result;

pub async fn query_remote(prompt: &str) -> Result<String> {
    let provider = OpenAIProvider::from_env();
    let model = Builder::new(provider).completion_model("gpt-3.5-turbo")?;
    let resp = model.complete(prompt, Default::default()).await?;
    Ok(resp.text)
}
Add .env:

ini
Copy
Edit
OPENAI_API_KEY=sk-...
üîç Summary
Project	Use Case	Key Crate
Local explain CLI	Run locally via GGML model	llama_cpp
Remote explain CLI	Use OpenAI API	llm (v1.3.2)

Would you like me to generate a ZIP or GitHub template for one or both of these projects to help you get started?










Sources

Ask ChatGPT

